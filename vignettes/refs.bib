
@article{lloyd_least_1982,
	title = {Least squares quantization in {PCM}},
	volume = {28},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/document/1056489},
	doi = {10.1109/TIT.1982.1056489},
	abstract = {It has long been realized that in pulse-code modulation ({PCM}), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for2{\textasciicircum}bquanta,b=1,2, {\textbackslash}cdots, 7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.},
	pages = {129--137},
	number = {2},
	journaltitle = {{IEEE} Transactions on Information Theory},
	author = {Lloyd, S.},
	urldate = {2024-07-11},
	date = {1982-03},
	note = {Conference Name: {IEEE} Transactions on Information Theory},
	file = {IEEE Xplore Abstract Record:/Users/nyxie/Zotero/storage/GV5ZPY25/1056489.html:text/html;Submitted Version:/Users/nyxie/Zotero/storage/SXWL5YNE/Lloyd - 1982 - Least squares quantization in PCM.pdf:application/pdf},
}


@article{morissette_k-means_2013,
	title = {The k-means clustering technique: General considerations and implementation in Mathematica},
	volume = {9},
	issn = {1913-4126},
	url = {http://www.tqmp.org/RegularArticles/vol09-1/p015},
	doi = {10.20982/tqmp.09.1.p015},
	shorttitle = {The k-means clustering technique},
	pages = {15--24},
	number = {1},
	journaltitle = {Tutorials in Quantitative Methods for Psychology},
	shortjournal = {{TQMP}},
	author = {Morissette, Laurence and Chartier, Sylvain},
	urldate = {2024-07-13},
	date = {2013-02-01},
	langid = {english},
	file = {Morissette and Chartier - 2013 - The k-means clustering technique General consider.pdf:/Users/nyxie/Zotero/storage/IA3W84MQ/Morissette and Chartier - 2013 - The k-means clustering technique General consider.pdf:application/pdf},
}


@inreference{noauthor_taxicab_2024,
	title = {Taxicab geometry},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Taxicab_geometry&oldid=1210894145},
	abstract = {Taxicab geometry or Manhattan geometry is geometry where the familiar Euclidean distance is ignored, and the distance between two points is instead defined to be the sum of the absolute differences of their respective Cartesian coordinates, a distance function (or metric) called the taxicab distance, Manhattan distance, or city block distance. The name refers to the island of Manhattan, or generically any planned city with a rectangular grid of streets, in which a taxicab can only travel along grid directions. In taxicab geometry, the distance between any two points equals the length of their shortest grid path. This different definition of distance also leads to a different definition of the length of a curve, for which a line segment between any two points has the same length as a grid path between those points rather than its Euclidean length.},
	booktitle = {Wikipedia},
	urldate = {2024-07-14},
	date = {2024-02-28},
	langid = {english},
	note = {Page Version {ID}: 1210894145},
	file = {Snapshot:/Users/nyxie/Zotero/storage/VMVED7E2/Taxicab_geometry.html:text/html},
}


@inreference{noauthor_chebyshev_2024,
	title = {Chebyshev distance},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Chebyshev_distance&oldid=1228754066},
	abstract = {In mathematics, Chebyshev distance (or Tchebychev distance), maximum metric, or L∞ metric is a metric defined on a real coordinate space where the distance between two points is the greatest of their differences along any coordinate dimension. It is named after Pafnuty Chebyshev.
It is also known as chessboard distance, since in the game of chess the minimum number of moves needed by a king to go from one square on a chessboard to another equals the Chebyshev distance between the centers of the squares, if the squares have side length one, as represented in 2-D spatial coordinates with axes aligned to the edges of the board. For example, the Chebyshev distance between f6 and e2 equals 4.},
	booktitle = {Wikipedia},
	urldate = {2024-07-14},
	date = {2024-06-13},
	langid = {english},
	note = {Page Version {ID}: 1228754066},
}


@inreference{noauthor_mahalanobis_2024,
	title = {Mahalanobis distance},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Mahalanobis_distance&oldid=1230540622},
	abstract = {The Mahalanobis distance is a measure of the distance between two points, introduced by P. C. Mahalanobis in 1936. The mathematical details of Mahalanobis distance has appeared in the Journal of The Asiatic Society of Bengal. Mahalanobis's definition was prompted by the problem of identifying the similarities of skulls based on measurements (the earliest work related to similarities of skulls are from 1922 and another later work is from 1927). The sampling distribution of Mahalanobis distance has been obtained by Professor R.C. Bose, under the assumption of equal dispersion.If each of these axes is re-scaled to have unit variance, then the Mahalanobis distance corresponds to standard Euclidean distance in the transformed space. The Mahalanobis distance is thus unitless, scale-invariant, and takes into account the correlations of the data set.},
	booktitle = {Wikipedia},
	urldate = {2024-07-14},
	date = {2024-06-23},
	langid = {english},
	note = {Page Version {ID}: 1230540622},
}


@inreference{noauthor_cosine_2024,
	title = {Cosine similarity},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Cosine_similarity&oldid=1233915832},
	abstract = {In data analysis, cosine similarity is a measure of similarity between two non-zero vectors defined in an inner product space. Cosine similarity is the cosine of the angle between the vectors; that is, it is the dot product of the vectors divided by the product of their lengths. It follows that the cosine similarity does not depend on the magnitudes of the vectors, but only on their angle. The cosine similarity always belongs to the interval. For example, in information retrieval and text mining, each word is assigned a different coordinate and a document is represented by the vector of the numbers of occurrences of each word in the document. Cosine similarity then gives a useful measure of how similar two documents are likely to be, in terms of their subject matter, and independently of the length of the documents.
The technique is also used to measure cohesion within clusters in the field of data mining.
One advantage of cosine similarity is its low complexity, especially for sparse vectors: only the non-zero coordinates need to be considered.
Other names for cosine similarity include Orchini similarity and Tucker coefficient of congruence; the Otsuka–Ochiai similarity (see below) is cosine similarity applied to binary data.},
	booktitle = {Wikipedia},
	urldate = {2024-07-14},
	date = {2024-07-11},
	langid = {english},
	note = {Page Version {ID}: 1233915832},
}



@article{dunn_fuzzy_1973,
	title = {A Fuzzy Relative of the {ISODATA} Process and Its Use in Detecting Compact Well-Separated Clusters},
	volume = {3},
	issn = {0022-0280},
	url = {https://doi.org/10.1080/01969727308546046},
	doi = {10.1080/01969727308546046},
	abstract = {Two fuzzy versions of the k-means optimal, least squared error partitioning problem are formulated for finite subsets X of a general inner product space. In both cases, the extremizing solutions are shown to be fixed points of a certain operator T on the class of fuzzy, k-partitions of X, and simple iteration of T provides an algorithm which has the descent property relative to the least squared error criterion function. In the first case, the range of T consists largely of ordinary (i.e. non-fuzzy) partitions of X and the associated iteration scheme is essentially the well known {ISODATA} process of Ball and Hall. However, in the second case, the range of T consists mainly of fuzzy partitions and the associated algorithm is new; when X consists of k compact well separated ({CWS}) clusters, Xi , this algorithm generates a limiting partition with membership functions which closely approximate the characteristic functions of the clusters Xi . However, when X is not the union of k {CWS} clusters, the limiting partition is truly fuzzy in the sense that the values of its component membership functions differ substantially from 0 or 1 over certain regions of X. Thus, unlike {ISODATA}, the “fuzzy” algorithm signals the presence or absence of {CWS} clusters in X. Furthermore, the fuzzy algorithm seems significantly less prone to the “cluster-splitting” tendency of {ISODATA} and may also be less easily diverted to uninteresting locally optimal partitions. Finally, for data sets X consisting of dense {CWS} clusters embedded in a diffuse background of strays, the structure of X is accurately reflected in the limiting partition generated by the fuzzy algorithm. Mathematical arguments and numerical results are offered in support of the foregoing assertions.},
	pages = {32--57},
	number = {3},
	journaltitle = {Journal of Cybernetics},
	author = {Dunn, J. C.},
	urldate = {2024-07-14},
	date = {1973-01-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01969727308546046},
}


@article{khan_novel_2017,
	title = {A novel sequence space related to L p \${\textbackslash}mathcal\{L\}\_\{p\}\$ defined by Orlicz function with application in pattern recognition},
	volume = {2017},
	issn = {1029-242X},
	url = {https://journalofinequalitiesandapplications.springeropen.com/articles/10.1186/s13660-017-1541-6},
	doi = {10.1186/s13660-017-1541-6},
	pages = {300},
	number = {1},
	journaltitle = {Journal of Inequalities and Applications},
	shortjournal = {J Inequal Appl},
	author = {Khan, Mohd Shoaib and Lohani, Qm Danish},
	urldate = {2024-07-14},
	date = {2017-12},
	langid = {english},
	file = {Full Text:/Users/nyxie/Zotero/storage/F43R4TA2/Khan and Lohani - 2017 - A novel sequence space related to L p \$mathcal L .pdf:application/pdf},
}


@inproceedings{pun_unique_2007,
	title = {Unique Distance Measure Approach for K-means ({UDMA}-Km) Clustering Algorithm},
	doi = {10.1109/tencon.2007.4429131},
	abstract = {Clustering technique in data mining has received a significant amount of attention from machine learning community in the last few years and become one of the fundamental research areas. Among the vast range of clustering algorithms,K-means is one of the most popular clustering algorithms. The basic principle of the K-means algorithm is to know how different distance measure is defined. It is a critical issue for {\textbackslash}bf K. -means users. For example, how can we select a unique distance measure method for an optimum clustering task? Our research provides a statistical based Unique Distance Measure Approach for {\textbackslash}bf K-means ({UDMA}-Km) to this issue. We consider 112 supervised datasets and measure the statistical data characteristics using central tendency measure. Those data characteristics are split using well known entropy method to generate the rules. Finally, the generated rules are used to select the unique distance measure for {\textbackslash}bf K-means algorithm. The experiment is conducted within 112 problems and 10 fold cross validation methods. The most significant contribution of our study is that a new algorithm was created and the new algorithm can be used and has been used to solve any clustering tasks very quickly and provide much better optimum performance.},
	pages = {1--4},
	booktitle = {{TENCON} 2007 - 2007 {IEEE} Region 10 Conference},
	author = {{PUN}, {WK} Daniel and {ALI}, {ABM} Shawkat},
	date = {2007},
	file = {Submitted Version:/Users/nyxie/Zotero/storage/VJM9G68E/PUN and ALI - 2007 - Unique Distance Measure Approach for K-means (UDMA.pdf:application/pdf},
}


@incollection{wang_modified_2006,
	title = {A Modified K-Means Clustering with a Density-Sensitive Distance Metric},
	abstract = {The K-Means clustering is by far the most widely used method for discovering clusters in data. It has a good performance on the data with compact super-sphere distributions, but tends to fail in the data organized in more complex and unknown shapes. In this paper, we analyze in detail the characteristic property of data clustering and propose a novel dissimilarity measure, named density-sensitive distance metric, which can describe the distribution characteristic of data clustering. By using this dissimilarity measure, a density-sensitive K-Means clustering algorithm is given, which has the ability to identify complex non-convex clusters compared with the original K-Means algorithm. The experimental results on both artificial data sets and real-world problems assess the validity of the algorithm.},
	pages = {544--551},
	booktitle = {Rough Sets and Knowledge Technology},
	author = {Wang, Ling and Bo, Liefeng and Jiao, Licheng},
	date = {2006},
	doi = {10.1007/11795131_79},
	file = {Submitted Version:/Users/nyxie/Zotero/storage/LHPTM4K3/Wang et al. - 2006 - A Modified K-Means Clustering with a Density-Sensi.pdf:application/pdf},
}



@inreference{noauthor_dunn_2018,
	title = {Dunn index},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://simple.wikipedia.org/w/index.php?title=Dunn_index&oldid=6123442},
	abstract = {The Dunn Index ({DI}) is a metric for judging a clustering algorithm. A higher {DI} implies better clustering. It assumes that better clustering means that clusters are compact and well-separated from other clusters.
There are many ways to define the size of a cluster and distance between clusters.
The {DI} is equal to the minimum inter-cluster distance divided by the maximum cluster size. Note that larger inter-cluster distances (better separation) and smaller cluster sizes (more compact clusters) lead to a higher {DI} value.
    \{{\textbackslash}displaystyle {DI}=\{{\textbackslash}frac \{{\textbackslash}min {\textbackslash}limits \_\{1{\textbackslash}leq i{\textbackslash}leq j{\textbackslash}leq m\}{\textbackslash}delta (C\_\{i\},C\_\{j\})\}\{{\textbackslash}max {\textbackslash}limits \_\{1{\textbackslash}leq k{\textbackslash}leq m\}{\textbackslash}Delta \_\{k\}\}\}\}},
	booktitle = {Simple English Wikipedia, the free encyclopedia},
	urldate = {2024-07-23},
	date = {2018-05-21},
	langid = {english},
	note = {Page Version {ID}: 6123442},
	file = {Snapshot:/Users/nyxie/Zotero/storage/NNZH4Z2X/Dunn_index.html:text/html},
}


@Manual{stats_package,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2024},
    url = {https://www.R-project.org/},
  }



@article{jain_data_2010,
	title = {Data clustering: 50 years beyond K-means},
	volume = {31},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865509002323},
	doi = {10.1016/j.patrec.2009.09.011},
	series = {Award winning papers from the 19th International Conference on Pattern Recognition ({ICPR})},
	shorttitle = {Data clustering},
	abstract = {Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering.},
	pages = {651--666},
	number = {8},
	journaltitle = {Pattern Recognition Letters},
	shortjournal = {Pattern Recognition Letters},
	author = {Jain, Anil K.},
	urldate = {2024-08-20},
	date = {2010-06-01},
	keywords = {Data clustering, Historical developments, King-Sun Fu prize, Perspectives on clustering, User’s dilemma},
	file = {Jain - 2010 - Data clustering 50 years beyond K-means.pdf:/Users/nyxie/Zotero/storage/FTHCD5GD/Jain - 2010 - Data clustering 50 years beyond K-means.pdf:application/pdf;ScienceDirect Snapshot:/Users/nyxie/Zotero/storage/MUBBZ8RH/S0167865509002323.html:text/html},
}


@incollection{deza_encyclopedia_2009,
	location = {Berlin, Heidelberg},
	title = {Encyclopedia of Distances},
	isbn = {978-3-642-00234-2},
	url = {https://doi.org/10.1007/978-3-642-00234-2_1},
	pages = {1--583},
	booktitle = {Encyclopedia of Distances},
	publisher = {Springer},
	author = {Deza, Michel Marie and Deza, Elena},
	editor = {Deza, Elena and Deza, Michel Marie},
	urldate = {2024-08-29},
	date = {2009},
	langid = {english},
	doi = {10.1007/978-3-642-00234-2_1},
	file = {Full Text PDF:/Users/nyxie/Zotero/storage/EEEZP3J8/Deza and Deza - 2009 - Encyclopedia of Distances.pdf:application/pdf},
}


@inproceedings{aggarwal_surprising_2001,
	location = {Berlin, Heidelberg},
	title = {On the Surprising Behavior of Distance Metrics in High Dimensional Space},
	isbn = {978-3-540-44503-6},
	doi = {10.1007/3-540-44503-X_27},
	abstract = {In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a effciency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used Lknorm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric L(1norm) is consistently more preferable than the Euclidean distance metric L(2norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the Lknorm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.},
	pages = {420--434},
	booktitle = {Database Theory — {ICDT} 2001},
	publisher = {Springer},
	author = {Aggarwal, Charu C. and Hinneburg, Alexander and Keim, Daniel A.},
	editor = {Van den Bussche, Jan and Vianu, Victor},
	date = {2001},
	langid = {english},
	keywords = {Confusion Matrice, Distance Metrics, High Dimensional Space, Manhattan Distance, Query Point},
	file = {Full Text PDF:/Users/nyxie/Zotero/storage/C5DASI94/Aggarwal et al. - 2001 - On the Surprising Behavior of Distance Metrics in .pdf:application/pdf},
}

@Manual{clust_package,
    title = {cluster: Cluster Analysis Basics and Extensions},
    author = {Martin Maechler and Peter Rousseeuw and Anja Struyf and Mia Hubert and Kurt Hornik},
    year = {2023},
    url = {https://CRAN.R-project.org/package=cluster},
    note = {R package version 2.1.6 --- For new features, see the 'NEWS' and the 'Changelog' file in the package source)},
  }

 @Manual{stats_package,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2024},
    url = {https://www.R-project.org/},
  }



@incollection{kaufman_leonard_partitioning_1990,
	title = {Partitioning Around Medoids (Program {PAM})},
	rights = {Copyright © 2005 John Wiley \& Sons, Inc.},
	isbn = {978-0-470-31680-1},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470316801.ch2},
	abstract = {The prelims comprise: Short Description of the Method How to Use the Program {PAM} Examples More on the Algorithm and the Program Related Methods and References},
	pages = {68--125},
	booktitle = {Finding Groups in Data},
	publisher = {John Wiley \& Sons, Ltd},
	author = {{Kaufman, Leonard} and {Rousseeuw, Peter J.}},
	urldate = {2024-09-03},
	date = {1990},
	langid = {english},
	doi = {10.1002/9780470316801.ch2},
	note = {Section: 2
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470316801.ch2},
	keywords = {central memory, graphical representation, medoids, partitioning around medoids, representative objects},
	file = {Snapshot:/Users/nyxie/Zotero/storage/CTQD8BB8/9780470316801.html:text/html},
}


@book{oneil_weapons_of,
author = {O'Neil, Cathy},
title = {Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy},
year = {2016},
isbn = {0553418815},
publisher = {Crown Publishing Group},
address = {USA},
abstract = {A former Wall Street quant sounds an alarm on the mathematical models that pervade modern life and threaten to rip apart our social fabricWe live in the age of the algorithm. Increasingly, the decisions that affect our liveswhere we go to school, whether we get a car loan, how much we pay for health insuranceare being made not by humans, but by mathematical models. In theory, this should lead to greater fairness: Everyone is judged according to the same rules, and bias is eliminated. But as Cathy ONeil reveals in this urgent and necessary book, the opposite is true. The models being used today are opaque, unregulated, and uncontestable, even when theyre wrong. Most troubling, they reinforce discrimination: If a poor student cant get a loan because a lending model deems him too risky (by virtue of his zip code), hes then cut off from the kind of education that could pull him out of poverty, and a vicious spiral ensues. Models are propping up the lucky and punishing the downtrodden, creating a toxic cocktail for democracy. Welcome to the dark side of Big Data. Tracing the arc of a persons life, ONeil exposes the black box models that shape our future, both as individuals and as a society. These weapons of math destruction score teachers and students, sort rsums, grant (or deny) loans, evaluate workers, target voters, set parole, and monitor our health. ONeil calls on modelers to take more responsibility for their algorithms and on policy makers to regulate their use. But in the end, its up to us to become more savvy about the models that govern our lives. This important book empowers us to ask the tough questions, uncover the truth, and demand change.}
}

@Article{testthat_package,
    author = {Hadley Wickham},
    title = {testthat: Get Started with Testing},
    journal = {The R Journal},
    year = {2011},
    volume = {3},
    pages = {5--10},
    url = {https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf},
  }


@online{noauthor_html_nodate,
	title = {{HTML} Validation Problems · Issue \#1648 · r-lib/roxygen2},
	url = {https://github.com/r-lib/roxygen2/issues/1648},
	abstract = {I'm in the process of submitting an R package to {CRAN}, but I'm encountering the following roxygen-related {NOTE} when running devtools::check(remote = {TRUE}, manual = {TRUE}). Any help would be very app...},
	titleaddon = {{GitHub}},
	urldate = {2024-09-16},
	langid = {english},
	file = {Snapshot:/Users/nyxie/Zotero/storage/7G7I7Z4A/1648.html:text/html},
}

