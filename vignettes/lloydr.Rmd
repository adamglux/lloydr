---
title: "lloydr"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{lloydr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: refs.bib
nocite: '@*'
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(lloydr)
```

# Introduction 

Lloyd's Algorithm, a technique developed by Stuart Lloyd in 1957, serves as a cornerstone in the realms of signal processing and data clustering. Originally designed to enhance pulse-code modulation, this algorithm has stepped beyond its initial application, becoming synonymous with k-means clustering. By employing an iterative process that assigns data points to the nearest centroids and subsequently recalculates these centroids based on the assigned points, Lloyd's Algorithm effectively minimizes within-cluster variance. This vignette delves into the mechanics of Lloyd's Algorithm, explores various distance measures that influence clustering outcomes, and presents the functionality of the `lloydr` package— a tool designed to facilitate flexible clustering analyses in R. 

# Background

## Lloyd's Algorithm 

Lloyd's algorithm, originally introduced by Stuart Lloyd in 1957, is a foundational method in the field of signal processing and data clustering (@lloyd_least_1982). Initially developed for optimising pulse-code modulation (PCM), the algorithm's primary goal was to minimize the distortion in signal representation by optimally placing quantization levels. Later, the algorithm was recognized for its broader applicability in clustering, particularly as a way to minimize the within-cluster variance. It is widely known now as *k*-means clustering, or just *k*-means. 

Lloyd's algorithm operates using an iterative process, involving the assignment of data points to the nearest cluster centroids and updating these centroids based on the mean of the assigned points. This process continues until convergence, leading to the partitioning of the data into clusters that are as compact and distinct as possible. Its simplicity and effectiveness have made Lloyd's algorithm a fundamental tool in machine learning and data analysis (@morissette_k-means_2013).

*K*-means is useful in exploratory data analysis and data mining, due its computational efficiency, as well as the ability to reduce data complexity. Elements, or units are considered similar as a means of their proximity, given an input space, and different metrics can be used to measure similarities (@morissette_k-means_2013).   

### Distance Measures in Lloyd's

The choice of distance measure can impact the quality and appropriateness of the resulting clusters, as it affects how clusters are formed and how centroids (cluster centres) are updated. A typical similarity metric is the Euclidean distance — straight-line distance between two points in Euclidean space — is given by: 

$$dE = \sqrt{\sum^n_{i=1}(c_i-x_i)^2}$$
Where *c* is the cluster centre, *x* is the comparison point at *i*, for all *k* units. 

### Alternative Distances Used in `lloydr` package

There are numerous distance measures, each designed to capture different aspects of similarity or dissimilarity between data points based on the type of data. While Euclidean distance measures the straight-line distance between points in a continuous space, Manhattan distance sums the absolute differences between coordinates, emphasizing grid-like paths. Cosine distance, in contrast, measures the angle between vectors, focusing on orientation rather than magnitude, making it suitable for text or high-dimensional data. Gower distance is particularly versatile, allowing for a combination of numerical, categorical, and binary variables by normalizing and aggregating individual component distances. 

While many distance measures exist to suit many different purposes, the four above are highlighted and available in this function. They are measured by: 

- Manhattan Distance (L1 Norm), the sum of the absolute differences between corresponding coordinates of points. 

$$dE = \sum_{i=1}^{n} |c_i - x_i|$$

- Cosine Distance, the angle between two non-zero vectors, emphasizing the orientation rather than magnitude.

$$Cd = 1 - \frac{\sum_{i=1}^{n} c_i x_i}{\sqrt{\sum_{i=1}^{n} x_i^2} \times \sqrt{\sum_{i=1}^{n} c_i^2}} ]$$

- Gower Distance, combines different types of variables (numerical, categorical, binary) by calculating the distance for each variable type and averaging.

$$Gd = S_{ij}= \frac{\sum^n_{i=1}w_{ijk}s_{ijk}}{\sum^n_{i=1}w_{ijk}}$$

For two objects *i* and *j* have *p* descriptors, where $w_{ijk}$ are non-negative weights usually set to 1 and $s_{ijk}$ is the similarity between the two objects regarding their *k*-th variable. If the variable is binary or ordinal, the values of $s_{ijk}$ are 0 or 1, with 1 denoting equality. If the variable is continuous, $s_{ijk} = 1 - \frac{|x_i - x_j|}{R_k}$ with $R_k$ being the range of the *k*-th variable [@noauthor_gowers_2024].

These distance measures are crucial because they highlight different dimensions of data, affecting how patterns, clusters, or relationships are identified, and the choice of distance metric can significantly impact the results of analysis, especially in clustering, classification, and recommendation systems.


### Dunn Index

The Dunn Index is a metric used to evaluate the quality of clustering by measuring the ratio of the minimum inter-cluster distance to the maximum intra-cluster distance [@dunn_fuzzy_1973]. It is calculated by identifying the smallest distance between points from different clusters (inter-cluster distance) and dividing that by the largest distance between points within the same cluster (intra-cluster distance). 

A higher Dunn Index indicates better clustering, with well-separated clusters and compact, dense clusters. It can be used to compare clustering results, by helping to determine the optimal number of clusters or assess the effectiveness of different clustering algorithms / distance metrics.

## Potential Problems using Euclidean Distance 

As a form of unsupervised machine learning, clustering problems can be difficult to track. However, various problems can be mitigated, while improvements to analysis and modelling can come from the use of different distance measures. Some problems that can occur with Euclidean distance measures include: 

- sensitivity to scale: if features are on different scales (e.g., height in centimetres and weight in kilograms), the feature with the larger scale can dominate the distance calculation .
- outliers in the data: Euclidean distance is sensitive to outliers because outliers have a large impact on the mean, which is used to update centroids.
- different cluster shapes: Euclidean distance assumes that clusters are spherical and of similar size, which may not be true in practice (@wang_modified_2006).
- non-Euclidean data: Euclidean distance may not be appropriate for non-Euclidean data, such as categorical data or data with complex relationships (such as text data) (@pun_unique_2007).
- high-dimensionality (Curse of Dimensionality): In high-dimensional spaces, the Euclidean distance can become less meaningful because the distances between points tend to become similar (@khan_novel_2017).

## Existing R packages / methods 

The `stats` package in R [@stats_package] has a built-in method for computing *k*-means called, `kmeans()`. This function performs *k*-means clustering, and returns an object of class 'kmeans'. This method has a `fitted()` S3 method, and allows users to specify the type of algorithm used to find clusters (`algorithm = c("Hartigan-Wong", "Lloyd", "Forgy", "MacQueen")`). Note: these alternative algorithms are beyond the scope of this project, but would be suitable for future research and updates to this package, later on. 

Though easy to implement, this function does not allow users to change the distance measure, which means that using it on non-numeric, or outlier data can cause problems.  

Another package is the `cluster` package [@clust_package] with several methods used for analysing different datasets. Of these, there is the `pam()` function which cluster around medoids [@kaufman_leonard_partitioning_1990], which can be more robust to outliers. This function allows users to specify a metric or a vector of metrics. An area of improvement for this package is increasing usability by detailing the output and class methods, which I hope to build in my package.


## Design Solution

The goal in this package is to address the problems above by allowing different distance metrics to address outliers, cluster shapes and non-Euclidean data. 
  
Users of this R package can expect to easily iterate between distance metrics using Lloyd's Algorithm. The goals of the package are:

- easily change distance measures by setting: `distance = "Euclidean"`
- output cluster metrics (clusters, centroids)
- allow users to easily compare clusters by comparing Dunn Index, [@dunn_fuzzy_1973] or processing time

# Package Functions 

This packages contains two functions available to the user: `calculate_clusters` and `compare_clusters`. 

**please note**: more examples can be found in help files and along with supplementary vignettes.

### Calculate clusters 

This function applies Lloyd's algorithm to cluster a dataset based on a specified distance metric (e.g., Euclidean, Manhattan, Cosine, or Gower). It returns a list containing cluster assignments, centroids, sum of squares metrics, and convergence details, with optional scaling of the data before clustering.


This function takes 3 main arguments, with another 3 set as defaults: 

- `df`	A dataframe
- `k`	 an integer number of clusters (must be > 1)
- `distance`	A distance measure
- `max.iter = 100`	maximum iterations for convergence
- `tol = 1e-04`	A numeric value specifying the tolerance for convergence.
- `scale = TRUE`	Logical. If TRUE, scales the data before clustering; if FALSE, does not scale.

Usage:

```
library(lloydr)
## sample data 
set.seed(501)
data <- data.frame(x = c(rnorm(20), rnorm(20,10,5), rnorm(20, 20, 5)),
                   y = c(rnorm(20), rnorm(20,10,5), rnorm(20, 20, 5)))


# using data scaling
cluster_values <- calculate_clusters(df = data, k = 3, distance = "euclidean")

#S3 object methods
#print(cluster_values)
summary(cluster_values)

#plot clusters and cluster centroids for scaled data
plot(cluster_values$data.scaled[,1], 
     cluster_values$data.scaled[,2], 
     col = cluster_values$cluster)
points(cluster_values$centers, col = 1:2, pch = 8, cex = 4)

# without scaling
cluster_values <- calculate_clusters(df = data, k = 3, distance = "euclidean", scale = FALSE)
summary(cluster_values)

#plot clusters and cluster centroids for non-scaled data
plot(cluster_values$data[,1], 
     cluster_values$data[,2], 
     col = cluster_values$cluster)
points(cluster_values$centers, col = 1:2, pch = 8, cex = 4)

```

### Compare clusters 

This function evaluates clustering results across different distance metrics and values of k for a given dataset. It returns a dataframe that summarizes metrics like computation time, convergence iterations, and the Dunn index for each combination of distance metric and cluster count.

This function takes 2 main arguments, with another 3 set as defaults: 

- `df`	A dataframe
- `k`	 A vector of integers representing the desired number of clusters (must be > 1).
- `max.iter = 100`	maximum iterations for convergence
- `tol = 1e-04`	A numeric value specifying the tolerance for convergence.
- `scale = TRUE`	Logical. If TRUE, scales the data before clustering; if FALSE, does not scale.

Usage: 

```
library(lloydr)
## sample data 
data <- data.frame(x = c(rnorm(20), rnorm(20,10,5), rnorm(20, 20, 5)),
                   y = c(rnorm(20), rnorm(20,10,5), rnorm(20, 20, 5)))

#compare clusters for scaled data
cluster_comparison <- compare_clusters(data, k = 2:3)
print(cluster_comparison)

#compare clusters for non-scaled data
cluster_comparison <- compare_clusters(data, k = 2:3, scale = FALSE)
print(cluster_comparison)
```

### Future Exploration

The area of *unsupervised* machine learning lives up to its namesake as an area ripe for exploration and discovery. In cluster analysis there are hierarchical methods like agglomerative and divisive, and non-hierarchical include centroid (the focus of this package), model, density, and grid based methods. Within each of these are a range of techniques. Understanding distance measures is a gateway towards unlocking the potential in each of these methods.  

____

# References





